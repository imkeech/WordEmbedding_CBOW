{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3ym1dZtiYMp",
        "outputId": "4c497b7d-7ffa-4091-9e66-88d951eeea6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.data.path.append('.')\n"
      ],
      "metadata": {
        "id": "_LuemwWWiaif"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"shakespeare.txt\"\n",
        "with open(path, 'r') as file:\n",
        "     data: str = file.read()\n",
        "\n",
        "data = re.sub(r'[,!?;-]', '.',data)\n",
        "data = word_tokenize(data)\n",
        "data = [token.lower() for token in data if token.isalpha() or token == '.']"
      ],
      "metadata": {
        "id": "D4Z5ua81iiBI"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The Number of tokens = {len(data)} \\n {data[:20]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHY4X2m4iiDp",
        "outputId": "553f20c5-a240-41f9-d946-559e8eea8198"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Number of tokens = 1084877 \n",
            " ['this', 'is', 'the', 'etext', 'file', 'presented', 'by', 'project', 'gutenberg', '.', 'and', 'is', 'presented', 'in', 'cooperation', 'with', 'world', 'library', '.', 'inc']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fdist = nltk.FreqDist(word for word in data)\n",
        "print(\"Size of vocabulary: \",len(fdist) )\n",
        "print(\"Most frequent tokens: \",fdist.most_common(20) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1XAshraiiGg",
        "outputId": "e1f27dc0-ea67-4e43-c9cc-f87c59200dd7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of vocabulary:  22892\n",
            "Most frequent tokens:  [('.', 197374), ('the', 27597), ('and', 26724), ('i', 22059), ('to', 19188), ('of', 18180), ('a', 14654), ('you', 13828), ('my', 12465), ('that', 11503), ('is', 10995), ('in', 10990), ('not', 9481), ('for', 8241), ('with', 7994), ('me', 7769), ('it', 7718), ('be', 7081), ('your', 6873), ('his', 6851)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dict(data: list[str]) -> tuple:\n",
        "    words = sorted(list(set(data)))\n",
        "    n = len(words)\n",
        "    idx = 0\n",
        "    word2Ind = {}\n",
        "    Ind2word = {}\n",
        "    for k in words:\n",
        "        word2Ind[k] = idx\n",
        "        Ind2word[idx] = k\n",
        "        idx += 1\n",
        "    return word2Ind, Ind2word\n",
        "\n",
        "\n",
        "word2Ind, Ind2word = get_dict(data)\n",
        "V = len(word2Ind)\n",
        "print(\"Size of vocabulary: \", V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hrBPLhliiIw",
        "outputId": "1bd1ee11-c342-4790-df54-02582458ebfc"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of vocabulary:  22892\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Index of the word 'king' :  \",word2Ind['king'] )\n",
        "print(\"Word which has index 2743:  \",Ind2word[2743] )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UMYe3BCiiLI",
        "outputId": "f55138c4-debe-46ef-d842-a589a56e0a6b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index of the word 'king' :   11116\n",
            "Word which has index 2743:   butcherly\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def init_parameters(N: int, V: int, random_seed: int= 1) -> dict:\n",
        "    np.random.seed(random_seed)\n",
        "    parameters = {}\n",
        "\n",
        "    parameters[\"W1\"] = np.random.rand(N, V)\n",
        "    parameters[\"b1\"] = np.zeros(shape=(N, 1))\n",
        "\n",
        "    parameters[\"W2\"] = np.random.rand(V, N)\n",
        "    parameters[\"b2\"] = np.zeros(shape=(V, 1))\n",
        "\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "OOLePqXukyfl"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_N = 4\n",
        "tmp_V = 10\n",
        "params = init_parameters(tmp_N,tmp_V)\n",
        "assert params['W1'].shape == ((tmp_N,tmp_V))\n",
        "assert params['W2'].shape == ((tmp_V,tmp_N))\n",
        "print(f\"W1 shape: {params['W1'].shape}\")\n",
        "print(f\"W2 shape: {params['W2'].shape}\")\n",
        "print(f\"b1 shape: {params['b1'].shape}\")\n",
        "print(f\"b2 shape: {params['b2'].shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Boqm3F9lk0uv",
        "outputId": "96780bf8-912f-49fe-9665-fff11daf3219"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W1 shape: (4, 10)\n",
            "W2 shape: (10, 4)\n",
            "b1 shape: (4, 1)\n",
            "b2 shape: (10, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Union\n",
        "\n",
        "def softmax(z: Union[float, List[float]]) -> Union[float, List[float]]:\n",
        "    y = np.exp(z) / np.sum(np.exp(z), 0, keepdims=True)\n",
        "\n",
        "    return y\n",
        "\n",
        "def sigmoid(z: Union[float, List[float]]) -> Union[float, List[float]]:\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "def relu(z: Union[float, List[float]]) -> Union[float, List[float]]:\n",
        "    return np.maximum(z, 0)"
      ],
      "metadata": {
        "id": "AUmkxLsck2iv"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp = np.array([[1,2,3],\n",
        "                [1,1,1]\n",
        "               ])\n",
        "tmp_sm = softmax(tmp)\n",
        "display(tmp_sm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "Qv4ztpluk5A2",
        "outputId": "282548af-ea61-4cc6-8b08-c25b6d4dd270"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[0.5       , 0.73105858, 0.88079708],\n",
              "       [0.5       , 0.26894142, 0.11920292]])"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_propagation(x: np.ndarray, params: dict) -> tuple:\n",
        "    z1 = np.dot(params['W1'], x) + params['b1']\n",
        "    h = relu(z1)\n",
        "\n",
        "    z2 = np.dot(params['W2'], h) + params['b2']\n",
        "    y_hat = z2\n",
        "\n",
        "    return y_hat, h"
      ],
      "metadata": {
        "id": "_nj5N8Bxk6dQ"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_x = np.array([[0,1,0,0,0,0,0,0,0,0]]).T\n",
        "tmp_z, tmp_h = forward_propagation(tmp_x, params)\n",
        "print(\"call forward_prop\")\n",
        "print()\n",
        "# Look at output\n",
        "print(f\"z has shape {tmp_z.shape}\")\n",
        "print(\"z has values:\")\n",
        "print(tmp_z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqYRu2mfk8ZW",
        "outputId": "702a5f11-626a-4899-950f-d84f66b95d21"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "call forward_prop\n",
            "\n",
            "z has shape (10, 1)\n",
            "z has values:\n",
            "[[1.82887324]\n",
            " [1.38466287]\n",
            " [0.60100484]\n",
            " [0.83284144]\n",
            " [1.37936774]\n",
            " [1.20420827]\n",
            " [1.26273995]\n",
            " [2.0149547 ]\n",
            " [1.10825153]\n",
            " [1.93910889]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_loss(y_hat: np.ndarray, y: np.ndarray,\n",
        "                       batch_size: int) -> Union[float, List[float]]:\n",
        "    logprobs = np.multiply(np.log(y_hat), y)\n",
        "    cost = -1/batch_size * np.sum(logprobs)\n",
        "    cost = np.squeeze(cost)\n",
        "    return cost"
      ],
      "metadata": {
        "id": "clqTV7vmk-9u"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_propagation(x: np.ndarray, y: np.ndarray, y_hat: np.ndarray, h: np.ndarray,\n",
        "                         params: dict, batch_size: int) -> dict:\n",
        "    grads_params = {}\n",
        "\n",
        "    l1 = np.dot(params['W2'].T ,(y_hat - y))\n",
        "\n",
        "    grads_params['W1'] = np.dot(l1, x.T) / batch_size\n",
        "    grads_params['W2'] = np.dot((y_hat - y), h.T) / batch_size\n",
        "\n",
        "    grads_params['b1'] = np.sum(l1, axis=1, keepdims= True) / batch_size\n",
        "    grads_params['b2'] = np.sum((y_hat - y), axis= 1, keepdims= True) / batch_size\n",
        "\n",
        "    return grads_params"
      ],
      "metadata": {
        "id": "yNNCZy26lAh-"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def get_idx(words, word2Ind):\n",
        "    idx = []\n",
        "    for word in words:\n",
        "        idx = idx + [word2Ind[word]]\n",
        "    return idx\n",
        "\n",
        "def pack_idx_with_frequency(context_words, word2Ind):\n",
        "    freq_dict = defaultdict(int)\n",
        "    for word in context_words:\n",
        "        freq_dict[word] += 1\n",
        "    idxs = get_idx(context_words, word2Ind)\n",
        "    packed = []\n",
        "    for i in range(len(idxs)):\n",
        "        idx = idxs[i]\n",
        "        freq = freq_dict[context_words[i]]\n",
        "        packed.append((idx, freq))\n",
        "    return packed\n",
        "\n",
        "def get_vectors(data, word2Ind, V, C):\n",
        "    i = C\n",
        "    while True:\n",
        "        y = np.zeros(V)\n",
        "        x = np.zeros(V)\n",
        "        center_word = data[i]\n",
        "        y[word2Ind[center_word]] = 1\n",
        "        context_words = data[(i - C) : i] + data[(i + 1) : (i + C + 1)]\n",
        "        num_ctx_words = len(context_words)\n",
        "        for idx, freq in pack_idx_with_frequency(context_words, word2Ind):\n",
        "            x[idx] = freq / num_ctx_words\n",
        "        yield x, y\n",
        "        i += 1\n",
        "        if i >= len(data) - C:\n",
        "            print(\"i is being set to\", C)\n",
        "            i = C\n",
        "\n",
        "def get_batches(data, word2Ind, V, C, batch_size):\n",
        "    batch_x = []\n",
        "    batch_y = []\n",
        "    for x, y in get_vectors(data, word2Ind, V, C):\n",
        "        if len(batch_x) < batch_size:\n",
        "            batch_x.append(x)\n",
        "            batch_y.append(y)\n",
        "        else:\n",
        "            yield np.array(batch_x).T, np.array(batch_y).T\n",
        "            batch_x = []\n",
        "            batch_y = []"
      ],
      "metadata": {
        "id": "p4NMb14wlDg2"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(data: list[str], word2Ind: dict, N: int,\n",
        "                     V: int, epochs: int, alpha: float= 0.03,\n",
        "                     random_seed: int= 282) -> dict:\n",
        "\n",
        "    param = init_parameters(N,V, random_seed=random_seed)\n",
        "\n",
        "    batch_size = 128\n",
        "    epoch = 0\n",
        "    C = 2\n",
        "\n",
        "    for x, y in get_batches(data, word2Ind, V, C, batch_size):\n",
        "\n",
        "        z, h = forward_propagation(x, param)\n",
        "        yhat = softmax(z)\n",
        "\n",
        "        cost = cross_entropy_loss(yhat, y, batch_size)\n",
        "        if ( (epoch+1) % 10 == 0):\n",
        "            print(f\"iters: {epoch + 1} cost: {cost:.6f}\")\n",
        "\n",
        "\n",
        "        grads = backward_propagation(x,y, yhat, h, param, batch_size)\n",
        "\n",
        "\n",
        "        param['W1'] = param['W1'] - alpha * grads['W1']\n",
        "        param['W2'] = param['W2'] - alpha * grads['W2']\n",
        "        param['b1'] = param['b1'] - alpha * grads['b1']\n",
        "        param['b2'] = param['b2'] - alpha * grads['b2']\n",
        "\n",
        "        epoch +=1\n",
        "        if epoch == epochs:\n",
        "            break\n",
        "        if epoch % 100 == 0:\n",
        "            alpha *= 0.66\n",
        "\n",
        "    return param"
      ],
      "metadata": {
        "id": "OqV3MK0hlGyO"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "C = 2\n",
        "N = 50\n",
        "word2Ind, Ind2word = get_dict(data)\n",
        "V = len(word2Ind)\n",
        "num_iters = 150\n",
        "print(\"Call gradient_descent\")\n",
        "params = gradient_descent(data, word2Ind, N, V, num_iters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ms-xxduXlJEO",
        "outputId": "91b0e008-73ae-4200-9e91-b62c07979658"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Call gradient_descent\n",
            "iters: 10 cost: 10.515535\n",
            "iters: 20 cost: 10.707387\n",
            "iters: 30 cost: 10.320237\n",
            "iters: 40 cost: 10.211154\n",
            "iters: 50 cost: 10.123117\n",
            "iters: 60 cost: 10.107501\n",
            "iters: 70 cost: 9.980620\n",
            "iters: 80 cost: 9.905205\n",
            "iters: 90 cost: 9.822018\n",
            "iters: 100 cost: 9.705264\n",
            "iters: 110 cost: 9.742620\n",
            "iters: 120 cost: 9.485970\n",
            "iters: 130 cost: 9.366885\n",
            "iters: 140 cost: 9.579672\n",
            "iters: 150 cost: 9.518378\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wizDBhPjlK4A"
      },
      "execution_count": 43,
      "outputs": []
    }
  ]
}